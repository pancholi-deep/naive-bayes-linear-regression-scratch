{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrWRvMGc1AvwGwj6Swjcrz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Naive Bayes algorithm implementation with multinomial density function from scratch."],"metadata":{"id":"TgwC55yB3fWB"}},{"cell_type":"markdown","source":["## Process:\n","\n","1.   Load the dataset.\n","2. Split the dataset into ten equal parts. These will be used for the ten iterations of the 10-fold cross validation.\n","\n","3. For each iteration of the cross validation, select one of the ten parts to be the test set and the remaining nine parts to be the training set.\n","\n","4. For each class (hobby) in the dataset, calculate the prior probability by dividing the number of instances in the training set that belong to that class by the total number of instances in the training set.\n","\n","5. For each feature (age, education, and job status), calculate the likelihood of that feature given each class by counting the number of instances in the training set that belong to that class and have that feature, and dividing by the total number of instances in the training set that belong to that class.\n","\n","6. Use Bayes' theorem to calculate the posterior probability of each class given the features in the test set.\n","\n","7. Classify each instance in the test set as the class with the highest posterior probability.\n","\n","8. Calculate the accuracy of the classification by comparing the predicted class to the actual class.\n","\n","9. Repeat steps 3-8 for each iteration of the cross validation, and average the accuracy across all iterations to get the final accuracy score."],"metadata":{"id":"qnpeR1_X5Lbw"}},{"cell_type":"markdown","source":["### Hayes-Roth Dataset"],"metadata":{"id":"bdQZD2tdIK12"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import KFold\n","\n","# Load the Hayes-Roth dataset\n","data = np.loadtxt('/content/hayes-roth.data', delimiter=',')\n","\n","# Split the data into features and labels\n","X = data[:, :-1]\n","y = data[:, -1]\n","\n","# Define the number of folds for cross-validation\n","k = 10\n","\n","# Define the prior probabilities of each class\n","classes, counts = np.unique(y, return_counts=True)\n","priors = counts / len(y)\n","\n","# Define the number of features and classes\n","n_features = X.shape[1]\n","n_classes = len(classes)\n","\n","# Define the parameters of the multinomial distribution for each feature and class\n","parameters = np.zeros((n_classes, n_features))\n","\n","# Split the data into k folds for cross-validation\n","kf = KFold(n_splits=k, shuffle=True)\n","\n","# Iterate over the folds\n","scores = []\n","for train_index, test_index in kf.split(X):\n","    # Split the data into training and testing sets\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Estimate the parameters of the multinomial distribution for each feature and class using the training data\n","    for i, c in enumerate(classes):\n","        X_c = X_train[y_train == c]\n","        parameters[i] = (X_c.sum(axis=0) + 1) / (X_c.sum() + n_features)\n","\n","    # Compute the log probabilities of each class given the test data\n","    log_probs = np.zeros((len(test_index), n_classes))\n","    for i, x in enumerate(X_test):\n","        for j, c in enumerate(classes):\n","            log_prob = np.log(priors[j])\n","            for k in range(n_features):\n","                log_prob += x[k] * np.log(parameters[j, k])\n","            log_probs[i, j] = log_prob\n","\n","    # Predict the class with the highest log probability\n","    y_pred = classes[np.argmax(log_probs, axis=1)]\n","\n","    # Compute the accuracy of the predictions\n","    accuracy = np.mean(y_pred == y_test)\n","    scores.append(accuracy)\n","\n","# Print the average accuracy over all the folds\n","print('Average accuracy: ', (sum(scores)/len(scores))*100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxFsqxZoCdpz","executionInfo":{"status":"ok","timestamp":1678553613796,"user_tz":-330,"elapsed":1434,"user":{"displayName":"Deep Pancholi","userId":"11575317922289191955"}},"outputId":"41884997-eda6-468a-e66d-05b08257fcdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy:  44.010989010989015\n"]}]},{"cell_type":"markdown","source":["### car evaluation dataset"],"metadata":{"id":"uNcSqD52Ovr0"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Load the car evaluation dataset\n","data = np.genfromtxt('car.data', delimiter=',', dtype=str)\n","\n","# Define the number of folds for cross-validation\n","k = 10\n","\n","# Split the data into features and labels\n","X = data[:, :-1]\n","y = data[:, -1]\n","\n","# Define the prior probabilities of each class\n","classes, counts = np.unique(y, return_counts=True)\n","priors = counts / len(y)\n","\n","# Define the number of features and classes\n","n_features = X.shape[1]\n","n_classes = len(classes)\n","\n","# Define the likelihood parameters for each feature and class\n","parameters = {}\n","for i in range(n_features):\n","    feature_values = np.unique(X[:, i])\n","    parameters[i] = {}\n","    for j, c in enumerate(classes):\n","        class_mask = (y == c)\n","        class_counts = np.zeros(len(feature_values))\n","        for k, value in enumerate(feature_values):\n","            feature_mask = (X[:, i] == value)\n","            class_counts[k] = np.sum(class_mask & feature_mask)\n","        parameters[i][j] = (class_counts + 1) / (np.sum(class_mask) + len(feature_values))\n","\n","# Split the data into k folds for cross-validation\n","fold_indices = np.array_split(np.random.permutation(len(y)), k)\n","\n","# Iterate over the folds\n","scores = []\n","for i in range(k):\n","    # Split the data into training and testing sets\n","    test_indices = fold_indices[i]\n","    train_indices = np.concatenate([fold_indices[j] for j in range(k) if j != i])\n","    X_train, y_train = X[train_indices], y[train_indices]\n","    X_test, y_test = X[test_indices], y[test_indices]\n","\n","    # Compute the log probabilities of each class given the test data\n","    log_probs = np.zeros((len(test_indices), n_classes))\n","    for j, c in enumerate(classes):\n","        class_prob = np.log(priors[j])\n","        for l, x in enumerate(X_test):\n","            feature_prob = 0\n","            for m in range(n_features):\n","                feature_values = np.unique(X_train[:, m])\n","                if x[m] in feature_values:\n","                    feature_prob += np.log(parameters[m][j][np.where(feature_values == x[m])])\n","                else:\n","                    feature_prob += np.log(1 / (np.sum(y_train == c) + len(feature_values)))\n","            log_probs[l, j] = class_prob + feature_prob\n","\n","    # Predict the class with the highest log probability\n","    y_pred = classes[np.argmax(log_probs, axis=1)]\n","\n","    # Compute the accuracy of the predictions\n","    accuracy = np.mean(y_pred == y_test)\n","    scores.append(accuracy)\n","\n","# Print the average accuracy over all the folds\n","print('Average accuracy:', np.mean(scores)*100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIt8kZL2HEuT","executionInfo":{"status":"ok","timestamp":1678553693742,"user_tz":-330,"elapsed":4675,"user":{"displayName":"Deep Pancholi","userId":"11575317922289191955"}},"outputId":"223d19be-e1c6-43b4-9fdd-503fed603334"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average accuracy: 87.15277777777779\n"]}]},{"cell_type":"markdown","source":["### Breast cancer dataset"],"metadata":{"id":"GtRwXWWQOsNA"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Load the breast cancer dataset\n","data = np.genfromtxt('breast-cancer.csv', delimiter=',', skip_header=True)\n","\n","# Define the number of folds for cross-validation\n","k = 10\n","\n","# Split the data into features and labels\n","X = data[:, :-1]\n","y = data[:, -1]\n","\n","# Define the prior probabilities of each class\n","classes, counts = np.unique(y, return_counts=True)\n","priors = counts / len(y)\n","\n","# Define the number of features and classes\n","n_features = X.shape[1]\n","n_classes = len(classes)\n","\n","# Define the mean and standard deviation of each feature for each class\n","means = np.zeros((n_features, n_classes))\n","stds = np.zeros((n_features, n_classes))\n","for i, c in enumerate(classes):\n","    class_mask = (y == c)\n","    means[:, i] = np.mean(X[class_mask, :], axis=0)\n","    stds[:, i] = np.std(X[class_mask, :], axis=0)\n","\n","# Split the data into k folds for cross-validation\n","fold_indices = np.array_split(np.random.permutation(len(y)), k)\n","\n","# Iterate over the folds\n","scores = []\n","for i in range(k):\n","    # Split the data into training and testing sets\n","    test_indices = fold_indices[i]\n","    train_indices = np.concatenate([fold_indices[j] for j in range(k) if j != i])\n","    X_train, y_train = X[train_indices], y[train_indices]\n","    X_test, y_test = X[test_indices], y[test_indices]\n","\n","    # Compute the log probabilities of each class given the test data\n","    log_probs = np.zeros((len(test_indices), n_classes))\n","    for j, c in enumerate(classes):\n","        class_prob = np.log(priors[j])\n","        for l, x in enumerate(X_test):\n","            feature_prob = 0\n","            for m in range(n_features):\n","                if stds[m, j] == 0:\n","                    # If the standard deviation is 0, assume a small value instead\n","                    feature_prob += np.log(np.exp(-0.5 * ((x[m] - means[m, j]) ** 2)) / 0.0001)\n","                else:\n","                    feature_prob += np.log(np.exp(-0.5 * ((x[m] - means[m, j]) ** 2) / (stds[m, j] ** 2)) / (np.sqrt(2 * np.pi) * stds[m, j]))\n","            log_probs[l, j] = class_prob + feature_prob\n","\n","    # Predict the class with the highest log probability\n","    y_pred = classes[np.argmax(log_probs, axis=1)]\n","\n","    # Compute the accuracy of the predictions\n","    accuracy = np.mean(y_pred == y_test)\n","    scores.append(accuracy)\n","\n","# Print the average accuracy over all the folds\n","print('Average accuracy:', np.mean(scores)*100)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2d1HmyYM-CH","executionInfo":{"status":"ok","timestamp":1678555310420,"user_tz":-330,"elapsed":50579,"user":{"displayName":"Deep Pancholi","userId":"11575317922289191955"}},"outputId":"ddcbdeb7-d9c2-42a6-a22f-2085333e4542"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-0e1416f03764>:50: RuntimeWarning: divide by zero encountered in log\n","  feature_prob += np.log(np.exp(-0.5 * ((x[m] - means[m, j]) ** 2)) / 0.0001)\n","<ipython-input-7-0e1416f03764>:52: RuntimeWarning: divide by zero encountered in log\n","  feature_prob += np.log(np.exp(-0.5 * ((x[m] - means[m, j]) ** 2) / (stds[m, j] ** 2)) / (np.sqrt(2 * np.pi) * stds[m, j]))\n"]},{"output_type":"stream","name":"stdout","text":["Average accuracy: 0.17543859649122806\n"]}]},{"cell_type":"markdown","source":["### References:\n","1. https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n","2. https://machinelearningmastery.com/k-fold-cross-validation/\n","\n","Datasets:\n","- Hayes-Roth Dataset (https://archive.ics.uci.edu/ml/datasets/Hayes-Roth)\n","- Car Evaluation Dataset (https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)\n","- Breast Cancer Dataset (https://archive.ics.uci.edu/ml/datasets/Breast+Cancer)"],"metadata":{"id":"wwL80j7XX-gm"}},{"cell_type":"code","source":[],"metadata":{"id":"C263JgmkOb4J"},"execution_count":null,"outputs":[]}]}